{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gau_tensorflow import GAUTransformer, AutoregressiveWrapper\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import mixed_precision\n",
    "\n",
    "import time, os, psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device = gpus[0])\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "process = psutil.Process(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "CONTEXT_SIZE = 512\n",
    "EMBEDDING_DIM = 128\n",
    "VOCAB_SIZE = 50257\n",
    "DEPTH = 6\n",
    "\n",
    "## Training\n",
    "LR = 1e-04\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = GAUTransformer(\n",
    "        emb_dim = EMBEDDING_DIM,\n",
    "        n_tokens = VOCAB_SIZE,\n",
    "        depth = DEPTH,\n",
    "        causal = True,\n",
    "        use_rotary_embs = True,\n",
    "        laplace_attn_fn = True\n",
    "    )\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate = LR,\n",
    "        decay_steps = EPOCHS,\n",
    "        end_learning_rate = 1e-1\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Lion(\n",
    "        learning_rate = lr_schedule,\n",
    "        weight_decay = 1e-02,\n",
    "        clipvalue = 0.75,\n",
    "        jit_compile = False\n",
    "    )\n",
    "\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True,\n",
    "        reduction = tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "\n",
    "print(f'Global Policy: {mixed_precision.global_policy().name}')\n",
    "print(f'Compute dtype: {model.compute_dtype}')\n",
    "print(f'Variable dtype: {model.variable_dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the model weights by a starting shape\n",
    "model.build(tf.TensorShape([None, CONTEXT_SIZE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikitext = load_dataset('wikitext', 'wikitext-2-v1', split = 'train')\n",
    "\n",
    "## Context size + extra shift token\n",
    "block_size = CONTEXT_SIZE + 1\n",
    "\n",
    "def group_examples(examples):\n",
    "    examples = tokenizer(examples['text'])\n",
    "    concatenated_examples = { key : sum(examples[key], []) for key in examples.keys() }\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        key : [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for key, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    ## Minor optimization possibility by leaving behind the labels\n",
    "    #result['labels'] = result['input_ids'].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "## Tokenize text\n",
    "tokenized_set = wikitext.map(\n",
    "    group_examples,\n",
    "    batched = True,\n",
    "    drop_last_batch = True,\n",
    "    remove_columns = ['text'],\n",
    "    num_proc = 4\n",
    ")\n",
    "\n",
    "print(f'Number of tokens: {len(tokenized_set) * block_size : ,}\\n\\n')\n",
    "print(tokenized_set)\n",
    "\n",
    "## Create the Prefetch dataset\n",
    "train_set = tokenized_set.to_tf_dataset(\n",
    "    columns = 'input_ids',\n",
    "    label_cols = [],\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    prefetch = tf.data.AUTOTUNE,\n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "## Cast to tf.uint16 to save memory\n",
    "train_set = train_set.map(lambda x : tf.cast(x, tf.uint16))\n",
    "\n",
    "print(train_set, end = '\\n\\n')\n",
    "print(train_set.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = EPOCHS\n",
    "\n",
    "input_signature = [\n",
    "    tf.TensorSpec(shape = (None, CONTEXT_SIZE), dtype = tf.uint16, name = 'tokens'),\n",
    "    tf.TensorSpec(shape = (None, CONTEXT_SIZE), dtype = tf.uint16, name = 'labels')\n",
    "]\n",
    "\n",
    "@tf.function(input_signature = input_signature, jit_compile = True)\n",
    "def train_step(tokens, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(tokens, training = True)\n",
    "        loss = tf.reduce_mean(model.loss(labels, logits))\n",
    "        scaled_loss = model.optimizer.get_scaled_loss(loss)\n",
    "    scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "    return loss, grads\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    ## Track time and loss\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## Iterate over the batches of the dataset\n",
    "    pbar = tqdm(train_set, desc = 'Epoch : ' + str(epoch+1) + '/' + str(n_epochs))\n",
    "    for step, batch in enumerate(pbar):\n",
    "\n",
    "        ## Shift tokens\n",
    "        tokens, labels = batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "        ## Forward pass\n",
    "        loss, grads = train_step(tokens, labels)\n",
    "\n",
    "        ## Update the model parameters\n",
    "        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        ## Track epoch\n",
    "        total_loss += loss\n",
    "        epoch_time = time.time() - start_time\n",
    "        pbar.set_postfix_str('Loss: {0:.2f}, Time: {1:.2f}s, Mem: {2:.2f}%'.format(\n",
    "            total_loss.numpy() / len(train_set), epoch_time, process.memory_percent()),\n",
    "            refresh = True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
